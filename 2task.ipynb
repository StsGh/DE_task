{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2.1\n",
    "На любом языке программирования реализовать реконсиляцию транзакций клиентов банка из двух источников (как минимум один - таблица БД, второй на выбор). \n",
    "В данных должен присутствовать уникальный ключ - uid/id на выбор;\n",
    "Реконсиляция должна быть масштабируема и применима к Big Data\n",
    "\n",
    "Дополнительно сделать реконсиляцию:\n",
    "\n",
    "- для разных типов данных (дата, текст, числа)\n",
    "- для числовых данных должна быть возможность сконфигурировать толеранс (допустимую погрешность в %)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import csv\n",
    "import vertica_python\n",
    "import configparser\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil import parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Создадим тестовую таблицу с банковскими транзакциями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сформируем датасет с мастер-данными\n",
    "start_date = datetime.now().date() - timedelta(days=1365)\n",
    "end_date = datetime.now().date()\n",
    "user_list = []\n",
    "for i in range(1,200000):\n",
    "    user_list.append(i)\n",
    "df = []\n",
    "while start_date != end_date:\n",
    "    for i in range(0,1000):\n",
    "        df.append([random.choice(user_list),start_date,round(random.uniform(1, 1000), 5)])\n",
    "    start_date = start_date + timedelta(days=1)\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузим в БД"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "path = os.path.join(os.getcwd(), 'config.ini')\n",
    "settings = config.read(path)\n",
    "connection_info = {'host': config['CONNECTION']['host'],\n",
    "                   'port': config['CONNECTION']['port'],\n",
    "                   'user': config['CONNECTION']['user'],\n",
    "                   'password': config['CONNECTION']['password'],\n",
    "                   'database': config['CONNECTION']['database']\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_df.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(df)\n",
    "connection = vertica_python.connect(**connection_info)\n",
    "cur = connection.cursor()\n",
    "with open('test_df.csv', \"rb\") as fs:\n",
    "    my_file = fs.read().decode('utf-8', 'ignore')\n",
    "    cur.copy(\n",
    "        \"\"\"COPY DB_DEFAULT.tb3_transactions\n",
    "        FROM STDIN PARSER fcsvparser(delimiter=',', header='true') \n",
    "        \"\"\",\n",
    "        my_file)\n",
    "    connection.commit()\n",
    "cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#сформируем датасет с данными для проверки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = vertica_python.connect(**connection_info)\n",
    "cur = connection.cursor()\n",
    "cur.execute(\"\"\"SELECT * FROM DB_DEFAULT.tb3_transactions WHERE transaction_date between '2019-11-10' and '2019-12-10' \"\"\")\n",
    "df_test = cur.fetchall()\n",
    "df_test = pd.DataFrame(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#сформируем csv для использования в процессе реконсиляции\n",
    "df_test.to_csv('test_data.csv', sep=',', header=False, index=False, encoding='utf-8', na_rep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пояснение к решению\n",
    "Перед разработкой решения было рассмотрено несколько вариантов реализации системы:</p>\n",
    "- решение для транзакционной БД</p>\n",
    "- решение для аналитической БД</p>\n",
    "<p>Ключевое отличие заключается в размере батча реконсилируемых данных. В первом случае это был бы небольшой набор данных от 1 до нескольких записей, так как должно соблюдаться условие поддержки потока данных максимально приближенному к реальному времени. Во втором случае такое решение не подошло бы в силу большой нагрузки на СУБД. Оптимальным решением здесь выступает выбор большого набора данных.</p>\n",
    "В силу характера предстоящей работы и возможных задач было принято решение в пользу второго варианта.</p>\n",
    "Для оптимизации запроса к СУБД было принято решение о реализации механизма предварительного анализа данных и выделение ключевого условия для фильтра в зависимости от характера данных. Выделены следующие основные сценарии использования:</p>\n",
    "- анализ определенного временного интервала</p>\n",
    "- анализ транзакций определенного пользователя</p>\n",
    "- анализ конкретных транзакций</p>\n",
    "\n",
    "В зависимости от сценария формируется оптимальный запрос к БД с целью выбора наиболее релевантных данных и снижения требований к оперативной памяти, увеличения скорости выполнения операции реконсиляции.</p>\n",
    "Возможные альтернативные варианты реализации - с использованием ресурсов СУБД. Тоже неплохой вариант с возможностью работы на больших объемах данных. Минус заключается в меньшей функциональности.</p>\n",
    "В текущем решении рассмотрены следующие возможные варианты расхождения данных:</p>\n",
    "- данные есть в БД, но отсутствуют в csv (учтено)</p>\n",
    "- данные отсутсвуют в БД, данные есть в csv (не учитывалось)</p>\n",
    "- данные есть в двух системах, но имеют разные значения (учтено)</p>\n",
    "- задвоенные данные в csv (учтено)</p>\n",
    "Вариант 2 не учитывался, так как БД была выбрана в качестве мастер-системы, с которой происходило сравнение. Данный вариант был выбран как  наиболее реалистичный.</p>\n",
    "***\n",
    "#### Масштабируемость решения обеспечивается:\n",
    "- возможностью включения новых источников данных (расширения методами get_web_api_data, get_xml_data ..)</p>\n",
    "- адаптивной логикой в зависимости от типов данных, что сглаживает нагрузку на систему в зависимости от характера реконсиляции</p>\n",
    "***\n",
    "#### Дополнительно:\n",
    "- Помимо этого были рассмотрены варианты использования хэшей в механизме реконсиляции (в частности был проработан MD5). Но в силу того, что необходимо учитывать толеранс при сравнении данных, хэширование значительно усложняло логику, что в итоге увеличивало скорость работы системы</p>\n",
    "- Были рассмотрены варианты встраивания моделей машинного обучения в логику сравнения. От данного решения пришлось отказаться, так как требуется максимально точное сравнение, тк речь идет о деньгах и банковских транзакциях. ML можно использовать в качестве рекомендательного механизма для решения задач в этой области.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import csv\n",
    "import vertica_python\n",
    "import configparser\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil import parser\n",
    "\n",
    "class DataReconsilation:\n",
    "    \n",
    "    __config = configparser.ConfigParser()\n",
    "    __path = os.path.join(os.getcwd(), 'config.ini')\n",
    "    __settings = __config.read(__path)\n",
    "    __connection_info = {'host': __config['CONNECTION']['host'],\n",
    "                         'port': __config['CONNECTION']['port'],\n",
    "                         'user': __config['CONNECTION']['user'],\n",
    "                         'password': __config['CONNECTION']['password'],\n",
    "                         'database': __config['CONNECTION']['database']\n",
    "                         }\n",
    "    \n",
    "    def __init__(self, __base_system):\n",
    "        self.__base_system = __base_system\n",
    "\n",
    "    def _get_mastersystem_data(self,num_id,how='date'):\n",
    "        \"\"\"Выгрузка данных из мастер-системы. В данном случае используется СУБД Vertica\n",
    "        \"\"\"\n",
    "\n",
    "        connection = vertica_python.connect(**self.__connection_info)\n",
    "        cur = connection.cursor()\n",
    "        sql = \"\"\"SELECT * FROM DB_DEFAULT.tb3_transactions WHERE {how} in {num_id}\"\"\"\n",
    "        if how=='date':\n",
    "            cur.execute(sql.format(how='transaction_date', num_id=num_id))\n",
    "        elif how=='client':\n",
    "            cur.execute(sql.format(how='user_id', num_id=num_id))\n",
    "        elif how=='transaction':\n",
    "            cur.execute(sql.format(how='transaction_id', num_id=num_id))\n",
    "        temp = cur.fetchall()\n",
    "        cur.close()\n",
    "        return pd.DataFrame(temp,columns=None)\n",
    "    \n",
    "    def get_csv_data(self, path):\n",
    "        \"\"\" Загрузка данных из csv источника\n",
    "        \"\"\"\n",
    "        df_csv = pd.read_csv(path,header=None)\n",
    "        return df_csv\n",
    "    \n",
    "    def __get_web_api_data(self):\n",
    "        \"\"\"Возможный вариант масштабирования решения через подключения нового источника\n",
    "        \"\"\"\n",
    "        return None\n",
    "    \n",
    "    def __get_xml_data(self):\n",
    "        \"\"\"Возможный вариант масштабирования решения через подключения нового источника\n",
    "        \"\"\"\n",
    "        return None\n",
    "    \n",
    "    def _get_last_job(self,job_type,row_count):\n",
    "        \"\"\" Выбор из технической таблицы id последнего запущенного job'а\n",
    "        \"\"\"\n",
    "        connection = vertica_python.connect(**self.__connection_info)\n",
    "        cur = connection.cursor()\n",
    "        cur.execute(\"\"\"SELECT max(job_id) FROM DB_DEFAULT.tb3_jobs\"\"\")\n",
    "        temp = cur.fetchall()\n",
    "        sql = \"\"\"INSERT INTO DB_DEFAULT.tb3_jobs(job_id,match_type,start_date,row_count) \n",
    "                 VALUES ({job},'{job_type}',now(),{row_count});\"\"\"\n",
    "        cur.execute(sql.format(job=temp[0][0]+1,job_type=job_type,row_count=row_count))\n",
    "        connection.commit()\n",
    "        cur.close()\n",
    "        return int(temp[0][0])\n",
    "    \n",
    "    def __upload_new_df(self,df,table_name):\n",
    "        \"\"\"Метод для загрузки реконсилированных данных\n",
    "        \"\"\"\n",
    "        connection = vertica_python.connect(**self.__connection_info)\n",
    "        cur = connection.cursor()\n",
    "        pd.DataFrame(df,columns=None).to_csv('temp_data.csv',\n",
    "                                             sep=',', header=False, index=False,\n",
    "                                             encoding='utf-8', na_rep=' ')\n",
    "        with open('temp_data.csv', \"rb\") as fs:\n",
    "            my_file = fs.read().decode('utf-8', 'ignore')\n",
    "            sql = \"\"\"COPY {table_name} FROM STDIN PARSER \n",
    "                     fcsvparser(delimiter=',', header='false')\"\"\"\n",
    "            cur.copy(sql.format(table_name=table_name),my_file)\n",
    "            connection.commit()\n",
    "        cur.close()\n",
    "    \n",
    "    def _get_match_list(self,how,df_csv):\n",
    "        \"\"\"Выбор значений для выгрузки данных из БД, формирование sql-запроса\n",
    "        \"\"\"\n",
    "        if how == 'transaction':\n",
    "            j=0\n",
    "        elif how == 'client':\n",
    "            j=1\n",
    "        elif how == 'date':\n",
    "            j=2\n",
    "            \n",
    "        new_list = \"('\" + str(df_csv[j].unique()[0]) + \"'\"\n",
    "        for i in range(1,len(df_csv[j].unique())):\n",
    "            new_list = new_list + ',' + \"'\" + str(df_csv[j].unique()[i]) + \"'\"\n",
    "        new_list = new_list + ')'\n",
    "        \n",
    "        return new_list\n",
    "    \n",
    "    def __find_duplicates(self,df_csv):\n",
    "        \"\"\"Поиск возможного дублирования в данных\n",
    "        \"\"\"\n",
    "        duplicates_list = df_csv[df_csv.groupby(0)[0].transform('size') > 1][0].drop_duplicates().values.tolist()\n",
    "        return duplicates_list\n",
    "    \n",
    "    def get_matches_num(self,how,df_csv,tolerance,match_type,job,rows,df_bad,df_good):\n",
    "        \"\"\"Реконсиляция числовых данных\n",
    "        \"\"\"\n",
    "        for i in range(len(df_csv)):\n",
    "            if df_csv[0][i] in rows[0].values:\n",
    "                match = rows[rows[0] == df_csv[0][i]]\n",
    "                if float(match[3])*(1-tolerance/100) <= float(df_csv[3][i]) <= float(match[3])*(1+tolerance/100) :\n",
    "                    df_good.append([job,df_csv[0][i],df_csv[1][i],df_csv[2][i],df_csv[3][i]])\n",
    "                else:\n",
    "                    df_bad.append([job,df_csv[0][i],df_csv[1][i],df_csv[2][i],df_csv[3][i]])\n",
    "            else:\n",
    "                df_bad.append([job,df_csv[0][i],df_csv[1][i],df_csv[2][i],df_csv[3][i]])\n",
    "\n",
    "        return df_bad,df_good\n",
    "    \n",
    "    def get_matches_date(self,how,df_csv,match_type,job,rows,df_bad,df_good):\n",
    "        \"\"\"Реконсиляция данных даты-времени\n",
    "        \"\"\"\n",
    "        for i in range(len(df_csv)):\n",
    "            if df_csv[0][i] in rows[0].values:\n",
    "                match = rows[rows[0] == df_csv[0][i]]\n",
    "                if parser.parse(df_csv[2][i]).date() == match[2].values[0] :\n",
    "                    df_good.append([job,df_csv[0][i],df_csv[1][i],df_csv[2][i],df_csv[3][i],])\n",
    "                else:\n",
    "                    df_bad.append([job,df_csv[0][i],df_csv[1][i],df_csv[2][i],df_csv[3][i]])\n",
    "            else:\n",
    "                df_bad.append([job,df_csv[0][i],df_csv[1][i],df_csv[2][i],df_csv[3][i]])\n",
    "\n",
    "        return df_bad,df_good\n",
    "    \n",
    "    def get_matches_string(self,how,df_csv,match_type,job,rows,df_bad,df_good):\n",
    "        \"\"\"Реконсиляция строковых данных\n",
    "        \"\"\"\n",
    "        for i in range(len(df_csv)):\n",
    "            if df_csv[0][i] in rows[0].values:\n",
    "                match = rows[rows[0] == df_csv[0][i]]\n",
    "                if str(df_csv[1][i]) == str(match[1].values[0]) :\n",
    "                    df_good.append([job,df_csv[0][i],df_csv[1][i],df_csv[2][i],df_csv[3][i]])\n",
    "                else:\n",
    "                    df_bad.append([job,df_csv[0][i],df_csv[1][i],df_csv[2][i],df_csv[3][i]])\n",
    "            else:\n",
    "                df_bad.append([job,df_csv[0][i],df_csv[1][i],df_csv[2][i],df_csv[3][i]])\n",
    "\n",
    "        return df_bad,df_good\n",
    "    \n",
    "    def get_matches_all(self,how,df_csv,tolerance,match_type,job,rows,df_bad,df_good):\n",
    "        \"\"\"Реконсиляция полной строки\n",
    "        \"\"\"\n",
    "        for i in range(len(df_csv)):\n",
    "            if df_csv[0][i] in rows[0].values:\n",
    "                match = rows[rows[0] == df_csv[0][i]]\n",
    "                if ((str(df_csv[1][i]) == str(match[1].values[0])) and \n",
    "                    (parser.parse(df_csv[2][i]).date() == match[2].values[0]) and \n",
    "                    (float(match[3])*(1-tolerance/100) <= float(df_csv[3][i]) <= float(match[3])*(1+tolerance/100))):\n",
    "                    df_good.append([job,df_csv[0][i],df_csv[1][i],df_csv[2][i],df_csv[3][i]])\n",
    "                else:\n",
    "                    df_bad.append([job,df_csv[0][i],df_csv[1][i],df_csv[2][i],df_csv[3][i]])\n",
    "            else:\n",
    "                df_bad.append([job,df_csv[0][i],df_csv[1][i],df_csv[2][i],df_csv[3][i]])\n",
    "\n",
    "        return df_bad,df_good\n",
    "    \n",
    "    def select_match_type(self,how,df_csv,match_type,tolerance):\n",
    "        \"\"\"Выбор способа раконсиляции\n",
    "        \"\"\"\n",
    "        job = self._get_last_job(match_type,len(df_csv))+1\n",
    "        new_list = self._get_match_list(how,df_csv)\n",
    "        rows = db._get_mastersystem_data(new_list,how=how)\n",
    "        df_good =[]\n",
    "        df_bad =[]\n",
    "        \n",
    "        if match_type == 'date':\n",
    "            df_bad,df_good = self.get_matches_date(how,df_csv,match_type,job,rows,df_bad,df_good)\n",
    "            \n",
    "        elif match_type == 'numeric':\n",
    "            df_bad,df_good = self.get_matches_num(how,df_csv,tolerance,match_type,job,rows,df_bad,df_good)\n",
    "            \n",
    "        elif match_type == 'string':\n",
    "            df_bad,df_good = self.get_matches_string(how,df_csv,match_type,job,rows,df_bad,df_good)\n",
    "        \n",
    "        elif match_type == 'all':\n",
    "            df_bad,df_good = self.get_matches_all(how,df_csv,tolerance,match_type,job,rows,df_bad,df_good)\n",
    "        \n",
    "        duplicates = self.__find_duplicates(df_csv)\n",
    "        for i in duplicates:\n",
    "            df_bad.append([job,i[0],i[1],i[2],i[3]])\n",
    "        \n",
    "        return df_bad,df_good\n",
    "    \n",
    "    def get_match(self,path,match_type,tolerance):\n",
    "        \"\"\" Основной метод, запускающий реконсиляцию и загружающий реконсилированные данные в БД\n",
    "            match_types = [date,numeric,string]\n",
    "            tolerance is a number between 1 and 100 (%)\n",
    "        \"\"\"\n",
    "        \n",
    "        df_csv = self.get_csv_data(path)\n",
    "        transaction_len = len(df_csv[0].unique())\n",
    "        client_len = len(df_csv[1].unique())\n",
    "        date_len = len(df_csv[2].unique())\n",
    "        \n",
    "        if transaction_len <= client_len and transaction_len <= date_len:\n",
    "            df_bad,df_good = self.select_match_type('transaction',df_csv,match_type,tolerance)\n",
    "                    \n",
    "        elif client_len < transaction_len and client_len <= date_len:\n",
    "            df_bad,df_good = self.select_match_type('client',df_csv,match_type,tolerance)\n",
    "                    \n",
    "        elif date_len <= client_len and date_len < transaction_len:\n",
    "            df_bad,df_good = self.select_match_type('date',df_csv,match_type,tolerance)\n",
    "            \n",
    "        self.__upload_new_df(df_bad,'DB_DEFAULT.tb3_transactions_bad')\n",
    "        self.__upload_new_df(df_good,'DB_DEFAULT.tb3_transactions_norm')\n",
    "\n",
    "        return print('количество отличающихся записей,размер датасета -',len(df_bad),len(df_csv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DataReconsilation('csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.get_match('test_data.csv',match_type='all',tolerance=1)\n",
    "%%time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2.2\n",
    "На любом языке программирования реализовать сервис по сбору агрегатов из таблицы БД с банковскими транзакциями.\n",
    "Агрегаты должны быть собраны из реконсилированных данных по каждому клиенту в разрезах дней, месяцев и общий итог.\n",
    "\n",
    "Прошу обратить внимание на то, что для задачи 2 пункт 2 агрегаты должны быть построены на реконсилированных данных. В противном случае задание считается заваленным."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AggregationCreating:\n",
    "    \n",
    "    __config = configparser.ConfigParser()\n",
    "    __path = os.path.join(os.getcwd(), 'config.ini')\n",
    "    __settings = __config.read(__path)\n",
    "    __connection_info = {'host': __config['CONNECTION']['host'],\n",
    "                         'port': __config['CONNECTION']['port'],\n",
    "                         'user': __config['CONNECTION']['user'],\n",
    "                         'password': __config['CONNECTION']['password'],\n",
    "                         'database': __config['CONNECTION']['database']\n",
    "                         }\n",
    "    \n",
    "    def __init__(self):\n",
    "        return None\n",
    "\n",
    "    def __get_raw_data(self):\n",
    "        \"\"\" Выбор данных для построения агрегатов. \n",
    "            Данные берутся из реконсилированных данных таблицы tb3_transactions_norm.\n",
    "        \"\"\"\n",
    "        connection = vertica_python.connect(**self.__connection_info)\n",
    "        cur = connection.cursor()\n",
    "        sql = \"\"\"select * from DB_DEFAULT.tb3_transactions_norm t1\n",
    "                 where t1.job_id in (select max(job_id) from DB_DEFAULT.tb3_jobs);\"\"\"\n",
    "        cur.execute(sql)\n",
    "        temp = cur.fetchall()\n",
    "        cur.close()\n",
    "        return pd.DataFrame(temp,columns=['job_id','transaction_id','user_id','transaction_date','amount_USD'])\n",
    "    \n",
    "    def get_day_agg(self):\n",
    "        \"\"\" Агрегация данных по дням\n",
    "        \"\"\"\n",
    "        df = self.__get_raw_data()\n",
    "        final = pd.DataFrame(df.groupby(['user_id', 'transaction_date'])['amount_USD'].sum().reset_index())\n",
    "        final.sort_values(by='user_id',inplace=True)\n",
    "        return final\n",
    "    \n",
    "    def get_month_agg(self):\n",
    "        \"\"\" Агрегация данных по месяцам\n",
    "        \"\"\"\n",
    "        df = self.__get_raw_data()\n",
    "        df['transaction_date2'] = df['transaction_date'].apply(lambda x: x.strftime('%Y-%m'))\n",
    "        final = pd.DataFrame(df.groupby(['user_id', 'transaction_date2'])['amount_USD'].sum().reset_index())\n",
    "        final.sort_values(by='user_id',inplace=True)\n",
    "        return final\n",
    "    \n",
    "    def get_total_agg(self):\n",
    "        \"\"\" Агрегация данных в тотале\n",
    "        \"\"\"\n",
    "        df = self.__get_raw_data()\n",
    "        final = pd.DataFrame(df.groupby(['user_id'])['amount_USD'].sum().reset_index())\n",
    "        final.sort_values(by='user_id',inplace=True)\n",
    "        return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db2 = AggregationCreating()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db2.get_total_agg()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
